# 课节
## 1 前言

## 2 预备知识
### 2.1数据操作
### 2.2数据预处理
### 2.3线性代数
### 2.4微分
### 2.5自动求导
### 2.6概率
### 2.7查阅文档

## 3 线性神经网络
### 3.1线性回归
### 3.2线性回归的从零开始实现
### 3.3线性回归的简洁实现
### 3.4softmax回归
### 3.5图像分类数据集
### 3.6softmax回归从零开始实现
### 3.7softmax回归的简洁实现

## 4 多层感知机
### 4.1多层感知机
### 4.2多层感知机的从零开始实现
### 4.3多层感知机的简洁实现
### 4.4模型选择、欠拟合和过拟合
### 4.5权重衰减
### 4.6暂退法（Dropout）
### 4.7前向传播、反向传播和计算图
### 4.8数值稳定性和模型初始化
### 4.9环境和分布偏移
### 4.10实战Kaggle比赛：预测房价

## 5 深度学习计算
### 5.1层和块
### 5.2参数管理
### 5.4自定义层
### 5.5读写文件
### 5.6GPU

## 6 卷积神经网络
### 6.1从全连接层到卷积
### 6.2图像卷积
### 6.3填充和步幅
### 6.4多输入多输出通道
### 6.5汇聚层
### 6.6卷积神经网络（LeNet）

## 7 现代卷积神经网络
### 7.1深度卷积神经网络（AlexNet）
### 7.2使用块的网络（VGG）
### 7.3网络中的网络（NiN）
### 7.4含并行连结的网络（GoogLeNet）
### 7.5批量规范化
### 7.6残差网络（ResNet）
### 7.7稠密连接网络（DenseNet）

## 8 循环神经网络
### 8.1.序列模型
### 8.2. 文本预处理
### 8.3. 语言模型和数据集
### 8.4. 循环神经网络
### 8.5. 循环神经网络的从零开始实现
### 8.6. 循环神经网络的简洁实现
### 8.7. 通过时间反向传播

## 9 现代循环神经网络
### 9.1门控循环单元（GRU）
### 9.2长短期记忆网络（LSTM）
### 9.3.深度循环神经网络
### 9.4.双向循环神经网络
### 9.5.机器翻译与数据集
### 9.6.编码器-解码器架构
### 9.7. 序列到序列学习（seq2seq）
### 9.8.束搜索

## 10 注意力机制
### 10.1. 注意力提示
### 10.2注意力汇聚：Nadaraya-Watson 核回归
### 10.3注意力评分函数
### 10.4Bahdanau 注意力
### 10.5多头注意力
### 10.6自注意力和位置编码
### 10.7Transformer

## 11 优化算法
### 11.1优化和深度学习
### 11.2凸性
### 11.3梯度下降
### 11.4随机梯度下降
### 11.5小批量随机梯度下降
### 11.6动量法
### 11.7. AdaGrad算法
### 11.8. RMSProp算法
### 11.9. Adadelta
### 11.10 adam

## 11 优化算法（续）
### 11.11 学习率调度器

## 12 计算性能
### 12.1编译器和解释器
### 12.2异步计算
### 12.3自动并行
### 12.4. 硬件
### 12.5 多gpu训练
### 12.6多GPU的简洁实现
### 12.7.参数服务器

## 13 计算机视觉
### 13.1图像增广
### 13.2微调
### 13.3. 目标检测和边界框
### 13.4. 锚框
### 13.5. 多尺度目标检测
### 13.6 目标检测数据集
### 13.7. 单发多框检测（SSD）
### 13.8区域卷积神经网络(R-CNN)系列
### 13.9. 语义分割和数据集
### 13.10.转置卷积

## 13 计算机视觉（续）
### 13.11. 全卷积网络
### 13.12.风格迁移
### 13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)
### 13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）

## 14 自然语言处理：预训练
### 14.1. 词嵌入（Word2vec）
### 14.2. 近似训练
### 14.3. 用于预训练词嵌入的数据集
### 14.4. 预训练word2vec
### 14.5. 全局向量的词嵌入（GloVe）
### 14.6. 子词嵌入
### 14.7：词的相似性和类比任务
### 14.8. 来自Transformers的双向编码器表示（BERT）
### 14.9. 用于预训练BERT的数据集
### 14.10. 预训练BERT

## 15 自然语言处理：应用
### 15.1. 情感分析及数据集
### 15.2.情感分析：使用循环神经网络
### 15.3.情感分析：使用卷积神经网络
### 15.4.自然语言推断与数据集
### 15.5.自然语言推断：使用注意力
### 15.6.针对序列级和词元级应用程序微调BERT
### 15.7.自然语言推断：微调BERT
